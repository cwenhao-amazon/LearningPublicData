[
  {
    "type": "tech_choices",
    "summary": "# Tech Choices Summary\n\nThis repository is primarily focused on speech recognition and language model evaluation, built with Python as the core programming language. It integrates multiple machine learning frameworks for speech recognition tasks, with a comprehensive evaluation system to benchmark different models across various datasets.\n\n## Programming Languages\n\n- **Python**: The primary programming language used throughout the codebase\n- Used for evaluation scripts, API integrations, and model implementations\n- Evident from numerous `.py` files across different directories\n\n## Backend Technologies\n\n- **Python with various ML frameworks**: The backend is built around Python-based machine learning systems\n- Includes PyTorch for deep learning (evident from torch imports and tensor operations)\n- Uses Hugging Face's Transformers library for model loading and processing\n- Integrates with multiple ASR (Automatic Speech Recognition) frameworks\n- Focuses on evaluating and benchmarking different speech recognition models\n\n## API Design Patterns\n\n- **REST API**: The codebase integrates with multiple third-party REST APIs\n- Authentication via API keys (OpenAI, AssemblyAI, ElevenLabs, Rev.ai)\n- HTTP requests to external services using the requests library\n- Handles HTTP responses and error codes\n- Implements retry logic for API failures\n- URL-based resource access patterns\n\n## Testing Frameworks\n\n- **Evaluate (HuggingFace)**: Used as the primary testing framework\n- Imports the evaluate library: `import evaluate`\n- Loads WER (Word Error Rate) metric: `wer_metric = evaluate.load(\"wer\")`\n- Computes metrics on test results\n- Focuses on evaluating speech recognition models by comparing transcriptions against reference texts\n- Calculates performance metrics like WER and RTFx (Real-Time Factor)\n- Provides standardized evaluation across different model implementations and datasets\n\n## Package Management\n\n- **pip**: Used for Python package management\n- Multiple requirements.txt files in the requirements directory\n- Separate requirement files for different components (trtllm, nemo, kyutai, etc.)\n\n## Machine Learning Frameworks\n\n- **SpeechBrain**: For speech recognition models including Conformer and CRDNN\n- **NVIDIA NeMo**: Used for ASR models like Fast Conformer RNNT\n- **Transformers**: Implements models like MMS, Wav2Vec2, and Whisper\n- **TensorRT-LLM**: Optimized implementation of Whisper\n- **CTTranslate2**: Another implementation framework for Whisper\n- **Kyutai**: Custom ML framework with dedicated scripts\n- **Phi**: Implementation of Phi4 multimodal models\n- **Granite**: Speech recognition framework with dedicated evaluation scripts\n\n## Version Control Systems\n\n- **Git**: Used for version control\n- Evidenced by the presence of .git directory and .gitignore file\n\nThe repository is structured around evaluating and comparing different speech recognition technologies, with a strong emphasis on benchmarking performance across various models and frameworks. The tech stack is heavily focused on machine learning tools specialized for speech processing tasks.",
    "data": null
  },
  {
    "type": "team_preferences",
    "summary": "# Team Preferences Summary\n\nThis document outlines the key working preferences and organizational approaches identified in the repository. The team appears to be focused on machine learning frameworks with a structured, modular approach to code organization and consistent evaluation methodologies.\n\n## Code Organization\n\nThe repository follows a framework-based organization structure with clear separation of concerns:\n\n- **Framework-specific directories**: Each ML framework has its own top-level directory\n  - speechbrain/\n  - nemo_asr/\n  - transformers/\n  - tensorrtllm/\n  - ctranslate2/\n  - kyutai/\n  - phi/\n  - granite/\n  \n- **Shared utilities**: Common functionality is extracted into separate directories\n  - normalizer/\n  - requirements/\n\nThis modular organization allows for clear separation between different ML frameworks while enabling code reuse through shared utility modules.\n\n## Version Control Workflows\n\nThe team uses Git for version control with standard hook samples present:\n\n- Pre-commit hooks for checking non-ASCII filenames and whitespace errors\n- Pre-merge-commit validation hooks\n- Pre-push validation hooks to prevent pushing WIP commits\n\nThese are standard Git hook samples that haven't been activated (still have .sample extension), suggesting the team may be using basic Git workflows without custom automation.\n\n## Coding Style Guidelines\n\nThe team follows comprehensive Python coding style guidelines:\n\n### File Organization\n1. License header at the top of each file\n2. Import statements organized in groups (standard library, third-party, local)\n3. Constants and global variables defined at module level\n4. Classes and functions follow in logical order\n\n### Naming Conventions\n1. Classes: PascalCase (e.g., `BasicTextNormalizer`, `EnglishNumberNormalizer`)\n2. Functions/Methods: snake_case (e.g., `remove_symbols`, `get_text`)\n3. Variables: snake_case (e.g., `audio_length`, `transcription_time`)\n4. Constants: UPPER_SNAKE_CASE (e.g., `ADDITIONAL_DIACRITICS`, `DATA_CACHE_DIR`)\n5. Private attributes/methods: prefixed with underscore (e.g., `_private_method`)\n\n### Formatting\n1. Indentation: 4 spaces (no tabs)\n2. Line length: ~80-100 characters\n3. Blank lines between logical sections\n4. Two blank lines before class definitions\n5. One blank line before method definitions\n\n### Comments and Documentation\n1. Docstrings for modules, classes, and functions using triple quotes\n2. Function docstrings include:\n   - Description\n   - Arguments section with parameter types\n   - Returns section with return types\n   - Examples (when appropriate)\n3. Inline comments for complex logic\n\n### Error Handling\n1. Specific exception types with descriptive messages\n2. Use of `ValueError` for invalid inputs\n3. Proper exception propagation\n\n### Type Annotations\n1. Type hints for function parameters and return values\n2. Use of typing module for complex types (e.g., `List`, `Optional`, `Union`)\n\n## Testing Philosophy\n\nThe team employs a **consistent evaluation approach across frameworks**. This is evidenced by the presence of `run_eval.py` files in almost every framework directory:\n\n- moonshine/run_eval.py\n- speechbrain/run_eval.py\n- nemo_asr/run_eval.py\n- liteASR/run_eval.py\n- granite/run_eval.py\n- api/run_eval.py\n- tensorrtllm/run_eval.py\n- transformers/run_eval.py\n- phi/run_eval.py\n- ctranslate2/run_eval.py\n- kyutai/run_eval.py\n\nThis standardized evaluation methodology ensures that all implementations are tested using similar criteria, enabling fair comparisons between different models and frameworks. This approach reflects a commitment to consistent quality assessment across the entire codebase.",
    "data": null
  },
  {
    "type": "non_functional_specs",
    "summary": "# Non-functional Specifications Summary\n\nThis document summarizes the identified non-functional specifications for the speech recognition system based on repository analysis. The project appears to be focused on optimizing speech recognition models with particular emphasis on performance metrics and resource utilization.\n\n## Performance Requirements\n\nThe codebase demonstrates clear performance requirements centered around two critical metrics:\n\n- **RTFx (Real-Time Factor)**: Measures processing speed relative to audio length\n  - Higher values indicate faster processing\n  - Calculated as: `audio_length_s / transcription_time_s`\n  - Consistently tracked and reported across evaluation scripts\n\n- **WER (Word Error Rate)**: Measures transcription accuracy\n  - Lower values indicate better accuracy\n  - Calculated as percentage of errors in transcription\n  - Primary quality metric for model evaluation\n\nSeveral optimization techniques are implemented to meet these performance targets:\n- GPU acceleration with strategic device placement\n- Batch processing for improved throughput\n- Mixed precision (float16/bfloat16) computation\n- Warmup steps to eliminate cold-start overhead\n- Torch compilation optimizations\n- Parallel processing with ThreadPoolExecutor\n- Memory optimizations including static KV caching\n\n## Memory/CPU Constraints\n\nThe system implements careful resource management strategies:\n\n- **GPU Memory Optimization**:\n  - Explicit memory fraction control (`kv_cache_free_gpu_memory_fraction=0.9`)\n  - Precision-controlled model loading to reduce memory footprint\n  - Build scripts with configured maximum batch sizes and sequence lengths\n\n- **Batch Processing Control**:\n  - Configurable batch sizes via command-line arguments\n  - Default batch sizes ranging from 16 to 64 depending on model complexity\n  - Dynamic batch handling for different architectures\n\n- **Precision Management**:\n  - Mixed precision training using `torch.float16` or `torch.bfloat16` when available\n  - Explicit precision control in TensorRT-LLM build parameters\n  - Precision selection based on hardware capabilities\n\n- **Resource Allocation**:\n  - Thread pool management with configurable thread counts\n  - Device placement control for multi-GPU environments\n\n- **Model Size Adaptability**:\n  - Support for various model sizes (tiny to large)\n  - Configurable parameters for input length, sequence length, and beam width\n\nThese specifications indicate a system designed to balance high performance with efficient resource utilization across different hardware configurations.",
    "data": null
  }
]