[
  {
    "type": "tech_choices",
    "summary": "# Tech Stack Summary\n\nThis repository contains a Kubernetes-based deployment for AI applications, primarily focused on deploying large language models with supporting web interfaces. The project uses Helm charts to manage deployments of AnythingLLM, OpenWebUI, and Text Generation Inference services.\n\n## Programming Languages\n\n- **JavaScript/Node.js**: Used for the backend services, particularly in AnythingLLM\n  - Configured with standard Node.js environment variables (`nodeEnv: \"production\"`)\n  - Uses typical Node.js server port configuration (port 3001)\n  - Follows Node.js application conventions for directory structure\n\n## Backend Technologies\n\n- **Node.js**: Powers the AnythingLLM application with JWT authentication and standard Node.js configuration\n- **Text Generation Inference (TGI)**: Hugging Face's inference service for deploying large language models\n  - Configured to run models like Meta-Llama-3-70B-Instruct\n  - Optimized for GPU-accelerated inference\n\n## Database Systems\n\n- **LanceDB**: Used as the vector database for AnythingLLM\n  - Specifically configured for storing and querying vector embeddings\n  - Supports retrieval-augmented generation (RAG) capabilities\n\n## API Design Patterns\n\n- **REST**: The applications expose RESTful APIs\n  - Health check endpoints at `/api/ping` for readiness and liveness probes\n  - Standard HTTP ports configuration (80, 8080, 3001)\n  - OpenAI-compatible API mentioned in OpenWebUI configuration\n\n## Infrastructure & Deployment\n\n- **Kubernetes**: Used as the container orchestration platform\n- **Helm**: Used for packaging and deploying the applications to Kubernetes\n  - Multiple charts for different components (apps/openwebui, apps/anythingllm, models)\n\n## Package Management\n\n- **Helm**: Functions as both the deployment tool and package manager for the Kubernetes applications\n  - Organizes dependencies and configurations through Chart.yaml files\n\n## CI/CD Tools\n\n- **GitHub Actions**: Used for continuous integration and deployment\n  - Configured specifically for release processes\n\n## Authentication/Security\n\n- **JWT**: Used for authentication in AnythingLLM\n  - Configured with auto-generated JWT secrets if not provided\n- **Kubernetes Secrets**: Used to store sensitive information\n  - Stores API keys for OpenAI and other services\n  - Implements base64 encoding for secret values\n\n## Machine Learning Frameworks\n\n- **Hugging Face Transformers**: Used for deploying and running large language models\n  - Configured to use models from Hugging Face Hub\n  - Optimized for GPU-accelerated inference\n  - Deployed using Text Generation Inference service\n\n## Version Control Systems\n\n- **Git**: Used for source code version control\n  - Standard Git configuration and ignore files present",
    "data": null
  },
  {
    "type": "team_preferences",
    "summary": "# Team Preferences Summary\n\nThis summary outlines the working style and organizational approach for a team managing a Kubernetes-focused repository containing Helm charts for various applications and models.\n\n## Code Organization\n\nThe repository follows a clear and structured organization pattern for Helm charts:\n\n- Separate charts for different applications and components\n- Main directory structure:\n  - `charts/apps/openwebui/` - OpenWebUI application charts\n  - `charts/apps/anythingllm/` - AnythingLLM application charts\n  - `charts/models/` - Model-specific charts\n\nThis organization demonstrates a modular approach, allowing for independent development and deployment of different components while maintaining a consistent structure across the repository.\n\n## Version Control Workflows\n\nThe team employs **GitHub Flow with automated Helm chart releases**:\n\n- Changes are merged to the main branch which triggers releases\n- Continuous delivery approach using GitHub Actions\n- Automated release process using helm/chart-releaser-action\n- Charts are automatically packaged from both the models directory and apps subdirectories\n\nThis workflow indicates a trunk-based development approach where the main branch serves as the source of truth, and releases are automated without manual intervention, streamlining the deployment process.\n\n## Coding Style Guidelines\n\nThe team follows comprehensive Helm chart coding standards:\n\n### File Structure and Organization\n- Consistent file naming with lowercase and hyphens (kebab-case)\n- Templates organized by component (main, mcpo)\n- Separation of schema files (values.schema.json) from values files (values.yaml)\n\n### JSON Schema Standards\n- JSON Schema draft-07 implementation\n- \"$schema\" reference included at the top of schema files\n- Detailed descriptions for all properties\n- Explicit marking of required fields\n- Proper type validation and enumerations\n\n### YAML Formatting\n- 2-space indentation\n- Grouped related configuration blocks\n- Comments explaining configuration sections\n- Consistent spacing between major sections\n\n### Naming Conventions\n- camelCase for property names\n- Descriptive, self-documenting names\n- Component-specific configuration prefixing\n\n### Configuration Structure\n- Organization by component hierarchy (main, mcpo)\n- Grouping of related settings (image, service, resources)\n- Consistent property ordering across similar components\n- Separation of user-configurable options from internal settings\n\n### Security Practices\n- Explicit security context definitions\n- Separation of secrets from regular configuration\n- Proper validation for sensitive fields\n\n### Resource Management\n- Defined resource requests and limits\n- Standardized units for resources (m for CPU, Mi/Gi for memory)\n\nThese comprehensive guidelines ensure consistency across the codebase, making it more maintainable and easier for team members to understand and contribute to the project.",
    "data": null
  },
  {
    "type": "non_functional_specs",
    "summary": "# Non-functional Specifications Summary\n\nThis document summarizes the non-functional specifications identified in the repository, focusing on key aspects that define the system's operational characteristics and constraints.\n\n## Performance Requirements\n\nThe system employs resource-based performance requirements with specific optimizations for ML models:\n\n- **Web Applications**:\n  - OpenWebUI: CPU (500m request/limit), Memory (1Gi request, 2Gi limit), Storage (10Gi)\n  - AnythingLLM: CPU (250m request, 500m limit), Memory (512Mi request, 1Gi limit), Storage (10Gi)\n  - Defined health check probes with specific timing parameters\n\n- **ML Models (TGI)**:\n  - GPU optimization with configurable NUM_SHARD parameter (set to 8 in examples)\n  - Hardware-specific configurations for high-end instances (xe9680-nvidia-h100)\n  - Automatic resource optimization: \"TGI will use the 'best' configuration for arguments to stress out existing resources and benefit from all available VRAM\"\n  - Specialized shared memory allocation: 1Gi for NVIDIA instances, up to 512Gi for AMD instances\n\n## Scalability Expectations\n\nThe system employs limited horizontal scaling with a primary focus on vertical scaling:\n\n- **Web Applications**:\n  - Fixed at 1 replica with no horizontal autoscaling\n  - Use of persistent storage which limits horizontal scaling capabilities\n  - AnythingLLM explicitly uses `strategy: type: Recreate` to ensure single pod storage access\n\n- **ML Models**:\n  - Configurable replicas with hardware dependency warning: \"you need to have access on Dell to as many instances as replicas you define\"\n  - Vertical scaling through GPU allocation (NUM_SHARD: 8)\n  - Hardware-specific optimizations for different GPU types\n\n## Security Standards\n\nSecurity is primarily managed through Kubernetes Secret resources:\n\n- **Sensitive Data Management**:\n  - API keys stored in Kubernetes Secrets (not in ConfigMaps or environment variables)\n  - Base64 encoding used for all secret values via the b64enc function\n  - Separate Secret resources for each application\n\n- **Secret Configuration**:\n  - Consistent naming pattern: `{{ include \"chartname.fullname\" . }}-secrets`\n  - Standard \"Opaque\" type for user-defined data\n  - Clear labeling with component identifiers\n\n## Memory/CPU Constraints\n\nThe system implements tiered resource allocation with application-specific constraints:\n\n- **Web UI Components**:\n  - OpenWebUI Main: CPU (500m fixed), Memory (1Gi request, 2Gi limit)\n  - OpenWebUI MCPO (optional): CPU (100m request, 500m limit), Memory (128Mi request, 512Mi limit)\n  - AnythingLLM: CPU (250m request, 500m limit), Memory (512Mi request, 1Gi limit)\n\n- **ML Model Components**:\n  - GPU constraints configurable via NUM_SHARD parameter\n  - Shared memory allocation varies by hardware:\n    - NVIDIA instances: 1Gi sizeLimit\n    - AMD instances: 256Gi sizeLimit (512Gi for deepseek model)\n\n## Network Requirements\n\nThe system defines both internal and external service exposure using HTTP/TCP:\n\n- **Service Types**:\n  - Web applications: Configurable service types (default: ClusterIP for internal access)\n  - ML models: LoadBalancer service type for external access\n\n- **Port Configurations**:\n  - OpenWebUI: Port 8080 internally, configurable external port\n  - AnythingLLM: Port 3001 internally, port 80 externally\n  - TGI models: Configurable port (default 80) for both internal and external\n\n- **Access Patterns**:\n  - All services use TCP protocol and expose HTTP endpoints\n  - ML models use nginx-ingress controller with path-based routing and prefix matching",
    "data": null
  }
]