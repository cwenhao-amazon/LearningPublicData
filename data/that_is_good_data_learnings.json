[
  {
    "type": "tech_choices",
    "summary": "# Tech Choices Summary\n\nThis repository is primarily a documentation and data collection project focused on tracking issues with NLP evaluation datasets. It uses a minimal tech stack centered around Markdown for documentation and CSV files for structured data storage, with a focus on integration with the HuggingFace ecosystem.\n\n## Programming Languages\n\nThe repository exclusively uses **Markdown** for documentation and content organization. All key files (README.md, CONTRIBUTING.md, dataset_issues.md, sample_issues.md) are written in Markdown, reflecting the project's focus on documenting and tracking issues rather than implementing code.\n\n## Database Systems\n\nThe project uses **CSV** files as its primary data storage format. Multiple CSV files are stored in the `reported_issues` directory (including MKQA.csv, PAWS-X.csv, XNLI.csv, ETHOS.csv, XSum.csv, DROP.csv), providing a simple, structured approach to storing dataset issue information.\n\n## Machine Learning Frameworks\n\nThe repository is designed to work with **HuggingFace Datasets**, a popular library for accessing and working with NLP datasets. While there's no explicit code showing framework usage, the documentation repeatedly references the HuggingFace Datasets library and includes links to datasets on the HuggingFace platform. The project appears to be focused on reporting issues with datasets commonly used within the HuggingFace ecosystem.\n\n## Version Control Systems\n\nThe project uses **Git** for version control, as evidenced by the presence of a `.git` directory and `.gitignore` file in the repository structure.",
    "data": null
  },
  {
    "type": "team_preferences",
    "summary": "# Team Preferences Summary\n\nThis repository is primarily a documentation-focused project dedicated to tracking and reporting issues in NLP evaluation datasets. The team has established specific workflows and standards for contributing to this collaborative effort, with an emphasis on accuracy and validation.\n\n## Code Organization\n\nThe repository follows a simple documentation-focused structure organized around markdown files rather than code implementation. The main components include:\n\n- `README.md` - Main documentation explaining the project purpose\n- `CONTRIBUTING.md` - Guidelines for contributing to the repository\n- `dataset_issues.md` - Page for tracking issues with entire datasets\n- `sample_issues.md` - Page for tracking issues with individual examples in datasets\n- `reported_issues/` - Directory containing CSV files for individual dataset issues\n\nThis organization reflects the repository's purpose as a community resource for reporting and tracking issues in NLP evaluation datasets.\n\n## Version Control Workflows\n\nThe team uses Git with standard hooks and an issue-based contribution system. The contribution workflow involves:\n\n1. Reporting dataset issues either through GitHub issues or through an external form\n2. Validation of reported issues before inclusion in the repository\n3. Standard Git hooks are present but appear to be default samples\n\nThere's no evidence of a specific branching strategy or PR template being used, suggesting a straightforward Git workflow.\n\n## Testing Philosophy\n\nThe team employs manual validation as their primary quality assurance approach. Key aspects include:\n\n- Double-checking every reported issue before adding it to the repository\n- Inviting community members to help with validation, especially for languages the maintainers cannot judge themselves\n- Focus on accuracy and reliability of reported dataset issues\n\nThis manual validation process ensures the quality and accuracy of the dataset issue reports.\n\n## Issue Style Guidelines\n\nThe repository has established a structured format for reporting dataset issues:\n\n1. **Required information:**\n   - Dataset name\n   - Link to the dataset source with version information\n   - For individual examples: index of problematic example, the example itself, and issue description\n   - For multilingual datasets: affected languages\n\n2. **Optional information:**\n   - How the issue could be fixed\n   - Whether the reporter wants credit for detecting the issue\n\n3. **Issue categorization:**\n   - Issues with individual examples (tracked in CSV files)\n   - Issues with entire datasets (requiring peer-reviewed paper evidence)\n\nThe `sample_issues.md` file demonstrates how reported issues are organized by dataset name with counts and affected languages, providing a clear template for contributors to follow.\n\n## Code Review Standards\n\nThe team implements a manual validation process for reported issues. While not traditional code review, this quality control mechanism ensures:\n\n- All reported issues are double-checked before being added to the repository\n- The focus is on accuracy and validity of dataset issue reports rather than code quality\n- Community involvement in the validation process is encouraged\n\nThis approach aligns with the repository's documentation-focused nature and collaborative community purpose.",
    "data": null
  },
  {
    "type": "non_functional_specs",
    "summary": "# Non-functional Specifications Summary\n\nThis repository appears to have minimal explicit non-functional specifications documented. Based on the available information, only one area has been identified with clear non-functional requirements.\n\n## Maintainability Goals\n\nThe repository emphasizes a community-driven approach to maintaining data quality and validity. Key aspects include:\n\n- **Issue Validation Process**: The project implements a double-checking mechanism for all reported issues before they are added to the repository\n- **Community Participation**: There is an active request for community members to help validate reported issues\n- **Language-Specific Validation**: Special emphasis is placed on seeking help for validating issues in languages that the core maintainers cannot assess themselves\n\nThis approach suggests that rather than relying on automated metrics or formal documentation requirements, the project prioritizes human review and community involvement as the primary maintainability strategy. The goal appears to be ensuring high-quality, validated dataset issue reports through collaborative effort.\n\n*Note: The repository does not contain explicit information about performance requirements, scalability expectations, security standards, browser/device compatibility, memory/CPU constraints, load testing parameters, caching strategies, logging requirements, monitoring thresholds, data retention policies, audit trail requirements, or network requirements.*",
    "data": null
  }
]