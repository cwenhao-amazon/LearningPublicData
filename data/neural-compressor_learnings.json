[
  {
    "type": "tech_choices",
    "summary": "# Tech Choices for Neural Compressor\n\nNeural Compressor is a project focused on machine learning model optimization that supports multiple ML frameworks. The codebase is primarily written in Python with C++ components for performance-critical engine operations. It provides adaptors for various ML frameworks, enabling model compression and optimization across different platforms.\n\n## Programming Languages\n\n- **Python**: Primary language used for the main codebase\n- **C++**: Used for performance-critical engine components in the executor directory\n\n## Frontend Frameworks\n\n- **PyTorch**: Supported through dedicated adaptor\n- **TensorFlow**: Supported through dedicated adaptor\n- **ONNX**: Supported through dedicated adaptor for ONNX models\n\n## Backend Technologies\n\n- **TensorFlow**: Serves as both frontend and backend technology\n- **PyTorch**: Serves as both frontend and backend technology\n- **ONNX Runtime**: Used for executing ONNX models\n- **MXNet**: Supported through dedicated adaptor\n\n## API Design Patterns\n\n- **REST**: Used for the UX component as evidenced by web router and server implementations\n\n## Infrastructure & Deployment\n\n- **Docker**: Used for containerization and deployment, with multiple Dockerfiles available for different configurations\n\n## Testing Frameworks\n\n- **pytest**: Used extensively for testing Python components\n- The repository contains numerous test files following pytest conventions\n\n## Build Systems\n\n- **CMake**: Used for building C++ components\n- **Make**: Used for higher-level build orchestration as shown by the root Makefile\n\n## Package Management\n\n- **pip**: Used for Python dependency management with requirements.txt files\n\n## CI/CD Tools\n\n- **GitHub Actions**: Used for continuous integration and delivery workflows\n\n## Machine Learning Frameworks\n\n- **TensorFlow**: Core supported ML framework\n- **PyTorch**: Core supported ML framework\n- **ONNX Runtime**: Supported for ONNX model optimization\n- **MXNet**: Additional supported ML framework\n\n## Version Control Systems\n\n- **Git**: Used for version control with standard Git configuration files",
    "data": null
  },
  {
    "type": "team_preferences",
    "summary": "# Team Preferences Summary\n\nThis summary outlines the working style and organizational approach of the Neural Compressor team based on repository analysis. The team follows a structured, quality-focused development process with clear guidelines for code organization, contributions, and reviews.\n\n## Code Organization\n\nThe Neural Compressor codebase follows a modular structure with clear separation of concerns. Different components are organized into dedicated directories:\n\n- `neural_compressor/adaptor/` \n- `neural_compressor/model/`\n- `neural_compressor/strategy/`\n- `neural_compressor/experimental/`\n\nThis organization reflects a thoughtful architectural approach that separates different functional areas of the codebase.\n\n## Version Control Workflows\n\nThe team uses a Pull Request based workflow with mandatory code review. This process is formally documented in `contributions.md` and `CODE_OF_CONDUCT.md`. Key aspects include:\n\n- Structured PR submission process\n- Detailed checklist of requirements before submission\n- Emphasis on code quality through reviews\n- Clear contributor behavior guidelines via Code of Conduct\n\nThis approach ensures all code changes are reviewed for quality and consistency before integration.\n\n## Coding Style Guidelines\n\nThe repository follows comprehensive coding style guidelines based on established industry standards with customizations:\n\n1. C++ Style:\n   - Based on Google C++ Style Guide\n   - 120 character column limit\n   - Explicit pointer alignment (not derived)\n   - 2-space indentation\n\n2. General Formatting:\n   - UTF-8 encoding\n   - LF line endings\n   - Space-based indentation (2 spaces default)\n   - Trailing whitespace trimming\n   - Final newline insertion\n\n3. Python-specific:\n   - 4-space indentation (different from other files)\n   - Follows Google's Python style guide\n\nThese guidelines are enforced through configuration files (`.clang-format` and `.editorconfig`) to maintain consistency across the codebase.\n\n## Code Review Standards\n\nThe team implements checklist-based reviews with clear style and testing requirements. Before sending pull requests, contributors must ensure their changes meet several requirements:\n\n1. Code quality requirements:\n   - Adherence to Python Coding Style (Google's style guide)\n   - Code must pass pylint checks\n   - Code must be cleaned using flake8 and autopep8\n\n2. Testing requirements:\n   - Unit tests must be added to cover new code\n   - All unit tests must pass\n\n3. PR documentation requirements:\n   - Detailed change summary\n   - Change motivation explanation\n   - Potential regression analysis\n   - Test information\n   - Environment information\n\nThis structured approach ensures consistent quality across contributions.\n\n## Testing Philosophy\n\nThe team emphasizes comprehensive unit and integration testing as evidenced by extensive test directories:\n- `test/`\n- `engine/test/`\n- `examples/`\n\nThis commitment to testing indicates a quality-focused development approach that values code reliability and correctness.\n\n## PR Style Guidelines\n\nPull requests follow a structured template with detailed sections:\n\n1. Change Summary - A detailed description of the changes made\n2. Change Motivation - Explanation of why the change is necessary\n3. Change Limit - Analysis of potential regressions\n4. Test Info - Steps to reproduce issues or test new features\n5. Environment Info - Details about the development environment\n\nThis structured approach ensures PRs contain all necessary information for effective review and helps maintain project quality.\n\n## Issue Style Guidelines\n\nThe team uses basic GitHub issues with additional support contact options. Users can:\n- Submit questions, feature requests, and bug reports to GitHub issues\n- Contact maintainers directly via email (neural_compressor.maintainers@intel.com)\n\nThe relatively simple approach to issue management suggests a focus on direct communication rather than complex issue tracking processes.\n\n## Commit Message Style Guidelines\n\nWhile there are no explicit commit message guidelines documented, the structured PR template suggests that commit messages should follow similar principles:\n- Clear, descriptive content\n- Provide context for changes\n- Explain motivation when appropriate\n\nThe emphasis on detailed documentation in PRs indicates the team values clear communication about code changes.",
    "data": null
  },
  {
    "type": "non_functional_specs",
    "summary": "# Non-functional Specifications for Neural Compressor\n\nNeural Compressor is an Intel project focused on model optimization techniques to improve inference performance across various hardware platforms. The repository emphasizes performance optimization, hardware compatibility, and distributed computing capabilities.\n\n## Performance Requirements\n\nNeural Compressor's primary focus is on model optimization for inference performance. The repository implements several techniques to achieve this:\n\n- **Quantization**: Converting model weights from higher precision (like FP32) to lower precision formats (like INT8) to improve inference speed\n- **Pruning**: Removing unnecessary connections in neural networks to reduce model size and computational requirements\n- **Benchmarking**: Tools to measure and compare performance improvements from optimization techniques\n\nThese optimization techniques are central to the project's purpose, enabling AI models to run more efficiently on deployment hardware.\n\n## Scalability Expectations\n\nThe project supports distributed training and inference capabilities, as evidenced by multiple test files:\n\n- Distributed TensorFlow dataloader testing\n- Distributed PyTorch training support\n- Distributed metrics evaluation\n\nThese components enable the framework to scale across multiple compute nodes, allowing users to leverage additional hardware resources for larger models or datasets.\n\n## Security Standards\n\nThe project follows Intel's corporate security practices:\n\n- Vulnerabilities should be reported directly to the Intel Security Center\n- Public disclosure of security issues (e.g., via GitHub issues) is discouraged\n- The project adheres to Intel's Vulnerability Handling Guidelines\n- A formal security policy document (security_policy.md) outlines the proper channels for reporting security concerns\n\nThis approach ensures security issues are handled through established corporate processes rather than through the open-source project's public channels.\n\n## Memory/CPU Constraints\n\nNeural Compressor is designed to optimize for various hardware platforms, with special attention to Intel architectures:\n\n- Support for Intel ITEX (Intel\u00ae Extension for TensorFlow)\n- Support for Intel IPEX (Intel\u00ae Extension for PyTorch)\n- Hardware-specific adaptors to leverage platform-specific optimizations\n\nThese adaptors allow the framework to take advantage of specialized hardware features while managing memory and CPU constraints effectively.\n\n## Load Testing Parameters\n\nThe project implements a configurable benchmarking system with several key parameters:\n\n- **Warmup phase**: Default of 5 iterations to stabilize performance before measurement\n- **Performance testing phase**: Default of 20 iterations for actual measurement\n- **Hardware utilization configuration**:\n  - cores_per_instance: 4 (in test configuration)\n  - num_of_instance: 2 (in test configuration)\n\nThe benchmark framework supports various testing scenarios:\n- Testing with batch sizes larger than dataset size\n- Testing with dataset size between batch size and 2\u00d7 batch size\n- Testing with dataset size between 2\u00d7 batch size and warmup \u00d7 batch size\n\nPerformance is measured in throughput (images/second) and can be configured either through YAML files or programmatically.\n\n## Logging Requirements\n\nThe project implements structured logging with configurable levels across multiple components:\n\n- Core logging functionality in `neural_compressor/utils/logger.py`\n- User experience (UX) specific logging in `neural_compressor/ux/utils/logger.py`\n\nThis suggests a comprehensive logging system that can be adjusted based on deployment needs and troubleshooting requirements.",
    "data": null
  }
]